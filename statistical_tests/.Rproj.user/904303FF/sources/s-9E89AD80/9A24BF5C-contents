---
title: "Effect of the temporal map updates parameter on the average CPU usage"
output:
  html_notebook: default
---

We need to analyse the impact of two possible values for the temporal map updates on the average CPU usage. We have chosen two values for the temporal updates, namely, `off`, which is used in the primary experiments, meaning that there will be no temporal updates, and `on`, meaning that the map will be updated every `0.5s`. First, let us load the necessary libraries:
```{r}
if (!require('tidyverse')) install.packages('tidyverse', dependencies = TRUE); library('tidyverse')
if (!require('ggpubr')) install.packages('ggpubr', dependencies = TRUE); library('ggpubr')
if (!require('rstatix')) install.packages('rstatix', dependencies = TRUE); library('rstatix')
if (!require('ggplot2')) install.packages('ggplot2', dependencies = TRUE); library('ggplot2')
if (!require('coin')) install.packages('coin', dependencies = TRUE); library('coin')
```

Next, we load the raw data output of the performed experiment:
```{r}
data <- read.csv(file = '../../raw_data/test_temporal_updates/run_table.csv')
data <- subset(data, select = c(X__run_id, temporal_updates, avg_cpu_util))
data$temporal_updates <- as.factor(data$temporal_updates)
data$avg_cpu_util <- as.numeric(data$avg_cpu_util)
```

Mean and standard deviation of the average CPU usage for both temporal updates values are displayed in the summary below:
```{r}
data %>%
  group_by(temporal_updates) %>%
  get_summary_stats(avg_cpu_util, type = "mean_sd")
```

We can visualize the distribution of the data for both temporal updates treatments via boxplot below:
```{r}
pdf(file = "../../figures/temporal_updates_effect/cpu-usage.pdf")

ggplot(data, aes(x = temporal_updates, y = avg_cpu_util, fill = temporal_updates)) +
  theme_bw() + 
  geom_boxplot(width = 0.9, outlier.size = 0.5) + 
  stat_summary(fun = mean, color = 'black', geom = 'point', shape = 5, size = 1.5) +
  labs(x = "Temporal updates", y = "Average CPU usage (%)") +
  theme(legend.position = 'none') +
  theme(
    axis.title.x = element_text(size = 26, family = 'sans'),
    axis.title.y = element_text(size = 26, family = 'sans'),
    axis.text.x = element_text(size = 24, family = 'sans'),
    axis.text.y = element_text(size = 24, family = 'sans')
  )

dev.off()
```

# Welch's t-test

We will apply the Welch's t-test to compare the means of both treatments. This is the default t-test in R and the safer alternative to standard Student's t-test, when we cannot assume that variances of both treatments are equal. However, we still need to check if other assumptions for applying this test are met. The first assumption of having independent observations, i.e. each subject belongs to only one group and there are no relationships between observations within the two groups, is satisfied inherently by the very way in which the experiment is conducted.

## No significant outliers

Next assumption that we need to check is if there are no significant outliers in neither of the two groups:
```{r}
data %>% 
  group_by(temporal_updates) %>%
  identify_outliers(avg_cpu_util)
```

As the result above shows, there is only one outlier, for run 20, but it is not considered extreme in terms of statistical significance. We can proceed with other assumptions.

## Normality

Next, we need to check if data in both groups stem from a normal distribution. To do so, we first need to draw QQ-plot for both groups:
```{r}
ggqqplot(data, x = "avg_cpu_util", facet.by = "temporal_updates")
```
Data points on both figures seem to be more or less aligned alongside the respective reference lines, except around the edges, hence we may assume that normality assumption is satisfied for both temporal updates treatment groups. Nevertheless, we need to perform Shapiro-Wilk test for each group to confirm the assumptions drawn from the QQ-plots:
```{r}
data %>%
  group_by(temporal_updates) %>%
  shapiro_test(avg_cpu_util)
```
As the results of the test show, p-values for both groups are higher than `0.05` reference value (`p = 0.576` and `p = 0.754`, respectfully), which means that we cannot reject the null-hypothesis stating that the data within both groups are normally distributed. Hereby, we conclude that the normality assumption for both groups is satisfied.

## Computation

Since we established that there are no extreme outliers and the data for both groups is normally distributed, we can proceed with the execution of Welch's t-test:
```{r}
data %>%
  t_test(avg_cpu_util ~ temporal_updates) %>%
  add_significance()
```
The resulting p-value of $3.66 \cdot 10^{-6}$ is lower than `0.05` reference value for the `0.95` confidence interval. This means that the obtained p-value is statistically significant and we can reject $H_0$, which states that the means of the average CPU usage for both temporal updates values are equal. We can hereby conclude that the temporal updates parameter has a statistically significant effect on the average CPU usage.

## Effect size

We establish with Welch's t-test results that the temporal updates parameter has a statistically significant effect on the average CPU usage. Now we need to perform Cohen's d test to estimate the magnitude of the said effect:

```{r}
data %>%
  cohens_d(avg_cpu_util ~ temporal_updates)
```
The resulting effect size of `d = -3.030126` is interpreted as _large_ in R, according to the Cohen's rule of thumb^[http://www.utstat.toronto.edu/~brunner/oldclass/378f16/readings/CohenPower.pdf]. According to the new and extended rules of thumb defined by Sawilowsky, the magnitude for this d-value is considered as _huge_^[https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1536&context=jmasm]. This is indeed clear from the distribution of data for both treatments in the boxplot above, where we can see that the average CPU usage mean when temporal updates are `off` is `30.734%`, while it is `31.824%` for temporal updates `on`. The resulting d-value is negative since the average CPU usage mean for `off` temporal updates group is lower than the one for temporal updates `on`.
