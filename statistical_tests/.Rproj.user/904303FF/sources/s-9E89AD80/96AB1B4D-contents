---
title: "Effect of the number of velocity samples parameter on the average CPU usage"
output:
  html_notebook: default
---

We need to analyse the impact of two possible values for the number of velocity samples on the average CPU usage. We have chosen two values for the number of velocity samples, namely, `10x20`, i.e. `10` translation and `20` rotation velocities, which is used in the primary experiments, and `20x40`, i.e. `20` translation and `40` rotation velocities. First, let us load the necessary libraries:
```{r}
if (!require('tidyverse')) install.packages('tidyverse', dependencies = TRUE); library('tidyverse')
if (!require('ggpubr')) install.packages('ggpubr', dependencies = TRUE); library('ggpubr')
if (!require('rstatix')) install.packages('rstatix', dependencies = TRUE); library('rstatix')
if (!require('ggplot2')) install.packages('ggplot2', dependencies = TRUE); library('ggplot2')
if (!require('coin')) install.packages('coin', dependencies = TRUE); library('coin')
```

Next, we load the raw data output of the performed experiment:
```{r}
data <- read.csv(file = '../../raw_data/test_velocity_samples/run_table.csv')
data <- subset(data, select = c(X__run_id, velocity_samples, avg_cpu_util))
data$velocity_samples <- as.factor(data$velocity_samples)
data$avg_cpu_util <- as.numeric(data$avg_cpu_util)
```

Mean and standard deviation of the average CPU usage for both velocity samples values are displayed in the summary below:
```{r}
data %>%
  group_by(velocity_samples) %>%
  get_summary_stats(avg_cpu_util, type = "mean_sd")
```

We can visualize the distribution of the data for both number of velocity samples treatments via boxplot below:
```{r}
pdf(file = "../../figures/velocity_samples_effect/cpu-usage.pdf")

ggplot(data, aes(x = velocity_samples, y = avg_cpu_util, fill = velocity_samples)) +
  theme_bw() + 
  geom_boxplot(width = 0.9, outlier.size = 0.5) + 
  stat_summary(fun = mean, color = 'black', geom = 'point', shape = 5, size = 1.5) +
  labs(x = "Number of translation and\nrotation velocity samples (#)", y = "Average CPU usage (%)") +
  theme(legend.position = 'none') +
  theme(
    axis.title.x = element_text(size = 30, family = 'sans'),
    axis.title.y = element_text(size = 30, family = 'sans'),
    axis.text.x = element_text(size = 28, family = 'sans'),
    axis.text.y = element_text(size = 28, family = 'sans')
  )

dev.off()
```

# Welch's t-test

We will apply the Welch's t-test to compare the means of both treatments. This is the default t-test in R and the safer alternative to standard Student's t-test, when we cannot assume that variances of both treatments are equal. However, we still need to check if other assumptions for applying this test are met. The first assumption of having independent observations, i.e. each subject belongs to only one group and there are no relationships between observations within the two groups, is satisfied inherently by the very way in which the experiment is conducted.

## No significant outliers

Next assumption that we need to check is if there are no significant outliers in neither of the two groups:
```{r}
data %>% 
  group_by(velocity_samples) %>%
  identify_outliers(avg_cpu_util)
```

As the result above shows, there are indeed no significant outliers thus we can proceed with other assumptions.

## Normality

Next, we need to check if data in both groups stem from a normal distribution. To do so, we first need to draw QQ-plot for both groups:
```{r}
ggqqplot(data, x = "avg_cpu_util", facet.by = "velocity_samples")
```
Data points on both figures seem to be slightly scattered around the reference line so we cannot confirm with a great certainty that the data in either of the velocity samples treatment groups is normally distributed. This is why we need to perform Shapiro-Wilk test for both groups to complement QQ-plots:
```{r}
data %>%
  group_by(velocity_samples) %>%
  shapiro_test(avg_cpu_util)
```
As the results of the test show, p-values for both groups are higher than `0.05` reference value (`p = 0.371` and `p = 0.402`, respectfully), which means that we cannot reject the null-hypothesis stating that the data within both groups are normally distributed. Hereby, we conclude that the normality assumption for both groups is satisfied.

## Computation

Since we established that there are no extreme outliers and the data for both groups is normally distributed, we can proceed with the execution of Welch's t-test:
```{r}
data %>%
  t_test(avg_cpu_util ~ velocity_samples) %>%
  add_significance()
```
The resulting p-value of $0.0463$ is lower than `0.05` reference value for the `0.95` confidence interval. This means that the obtained p-value is statistically significant and we can reject $H_0$, which states that the means of the average CPU usage for both number of velocity samples values are equal. We can hereby conclude that the number of velocity samples parameter has a statistically significant effect on the average CPU usage.

## Effect size

We establish with Welch's t-test results that the number of velocity samples parameter has a statistically significant effect on the average CPU usage. Now we need to perform Cohen's d test to estimate the magnitude of the said effect:

```{r}
data %>%
  cohens_d(avg_cpu_util ~ velocity_samples)
```
The resulting effect size of `d = -0.9574514` is interpreted as _large_ in R, according to the Cohen's rule of thumb^[http://www.utstat.toronto.edu/~brunner/oldclass/378f16/readings/CohenPower.pdf]. According to the new and extended rules of thumb defined by Sawilowsky, the magnitude for this d-value is also considered as _large_^[https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1536&context=jmasm]. This is indeed clear from the distribution of data for both treatments in the boxplot above, where we can see that the average CPU usage mean for `10x20` velocity samples is `30.919%`, while it is `31.269%` for `20x40` velocity samples. The resulting d-value is negative since the average CPU usage mean for `10x20` velocity samples group is lower than the one for `20x40` samples.

