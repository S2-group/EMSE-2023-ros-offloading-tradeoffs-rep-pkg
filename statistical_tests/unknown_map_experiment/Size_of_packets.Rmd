---
title: "Unknown map experiment - total number of network packets"
output:
  html_notebook: default
---

We are analyzing if offloading any of the three functionalities (i.e., SLAM, navigation and object recognition) has a statistically significant effect on the total size of network packets. Fist, install and load the required libraries:
```{r}
if (!require('tidyverse')) install.packages('tidyverse', dependencies = TRUE); library('tidyverse')
if (!require('ggpubr')) install.packages('ggpubr', dependencies = TRUE); library('ggpubr')
if (!require('rstatix')) install.packages('rstatix', dependencies = TRUE); library('rstatix')
if (!require('ggplot2')) install.packages('ggplot2', dependencies = TRUE); library('ggplot2')
if (!require('lmPerm')) install.packages('lmPerm', dependencies = TRUE); library('lmPerm')
```

Next, load the experiment results from the csv file:
```{r}
data <- read.csv(file = '../../raw_data/unknown_map_experiment/run_table.csv')
data$size_of_packets <- data$size_of_packets / 1000000
data <- subset(data, select = c(X__run_id, slam_offloaded, navigation_offloaded, obj_recognition_offloaded, size_of_packets))
data$slam_offloaded <- as.factor(data$slam_offloaded)
data$navigation_offloaded <- as.factor(data$navigation_offloaded)
data$obj_recognition_offloaded <- as.factor(data$obj_recognition_offloaded)
data$size_of_packets <- as.numeric(data$size_of_packets)
```

Mean and standard deviation of the total number of network packets for each treatment are displayed in the summary below:
```{r}
data %>%
  group_by(slam_offloaded, navigation_offloaded, obj_recognition_offloaded) %>%
  get_summary_stats(size_of_packets, type = "mean_sd")
```

# Three-Way ANOVA test

Since there are three independent variables in the experiment, we opt for three-way ANOVA test. First, we need to check if all assumptions for performing the test are met. The fist assumption of having independent observations is met inherently according to the way the experiment itself is conducted. 

## No extreme outliers assumption

Next, we need to check if there are any extreme outliers:

```{r}
data %>%
  group_by(slam_offloaded, navigation_offloaded, obj_recognition_offloaded) %>%
  identify_outliers(size_of_packets)
```

The results show that there are six outliers, in runs 80, 68, 70, 55, 72 and 37, respectfully, while runs 68, 70, 55, 72 and 37 represent extreme outliers. We should remove the said runs from the dataset, but we can also leave them if we beleive that they will not have a significant effect on the obtained results. Let us make an alternative dataset with the extreme outliers being removed:

```{r}
data_without_outliers <- data
data_without_outliers <- data_without_outliers[!(data_without_outliers$X__run_id == 'run_60'),]
data_without_outliers <- data_without_outliers[!(data_without_outliers$X__run_id == 'run_70'),]
data_without_outliers <- data_without_outliers[!(data_without_outliers$X__run_id == 'run_55'),]
data_without_outliers <- data_without_outliers[!(data_without_outliers$X__run_id == 'run_72'),]
data_without_outliers <- data_without_outliers[!(data_without_outliers$X__run_id == 'run_37'),]

```

We proceed to check the next assumption - normality of the data. 

## Normality assumption by analyzing the model residuals

The first way to check normality assumption is by analyzing the model residuals:

```{r}
model  <- lm(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded, data = data)
ggqqplot(residuals(model))
```

The QQ plot in the figure above shows that most of the residuals are scattered around the reference line, especially around the edges, thus we cannot assume that the residuals are normally distributed. We will complement the QQ plot with Shapiro-Wilk test to conclude if the normality is indeed satisfied:
```{r}
shapiro_test(residuals(model)) %>%
  add_significance()
```
The p-value of $2.974 \cdot 10^{-8}$ shows that we can reject $H_0$, which states that the data is indeed normally distributed. We can conclude that the normally constraint is not satisfied. If we try with the dataset that does not contain significant outliers:
```{r}
model_without_outliers  <- lm(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded, data = data_without_outliers)
shapiro_test(residuals(model_without_outliers)) %>%
  add_significance()
```
we get very similar results that lead to the conclusion that normality is not satisfied.

## Normality assumption by each group

The other way to check normality is by analyzing if samples for each possible treatment combination are normally distributed:
```{r}
data %>%
  group_by(slam_offloaded, navigation_offloaded, obj_recognition_offloaded) %>%
  shapiro_test(size_of_packets) %>%
  add_significance()
```

Shapiro-Wilk test per each group yields p-value higher than `0.05` for only half of the groups. If we execute the test on the dataset without extreme outliers:
```{r}
data_without_outliers %>%
  group_by(slam_offloaded, navigation_offloaded, obj_recognition_offloaded) %>%
  shapiro_test(size_of_packets) %>%
  add_significance()
```
only the two groups yield significant p-values that are lower than `0.05` thus indicating non-normal distribution. The QQ-plots per group are depicted below, where the data indeed seems to be scattered around the reference line in the four groups that yielded statistically significant p-values in the Shapiro-Wilk test results:

```{r}
ggqqplot(data, "size_of_packets", ggtheme = theme_bw()) +
  facet_grid(slam_offloaded + navigation_offloaded ~ obj_recognition_offloaded, labeller = "label_both")
```

## Homogeneity of variance assumption

Homogeneity of variance assumption can be checked via Leveneâ€™s test:

```{r}
data %>% 
  levene_test(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded) %>%
  add_significance()
```
Since the p-value is lower than `0.05`, the result of the Levene's test is statistically significant, hence the null hypothesis stating that homogeneity of variance holds is rejected. If we compute the Levene's test on dataset without extreme outliers:
```{r}
data_without_outliers %>% 
  levene_test(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded) %>%
  add_significance()
```
we can draw the same conclusion - homogeneity of variance assumption is not satisfied. With the disputably normality constraint and no homogeneity of variance, we need to turn to non-parametric alternative of three-way ANOVA - permutation test.

# Permutation test

Execution of permutation test yields the following result:
```{r}
summary(aovp(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded, data = data, perm="Prob", maxIter=500000, nCycle=100, Ca=0.001))
```
Test results show that the main effects is significant only when object recognition is offloaded, but there is also a significant two-way interaction between navigation and object recognition. If we compare these results with three-way ANOVA results, that is considered as _robust_ when sample sizes are equal for each group:
```{r}
data %>% 
  anova_test(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded)

```
we see that the conclusions are very similar to those drawn from the permutation test and the obtained p-values are similar as well. The eta-squared effect sizes are:
```{r}
res.aov <- aov(size_of_packets ~ slam_offloaded*navigation_offloaded*obj_recognition_offloaded, data = data)
eta_squared(res.aov)
```

## Pairwise comparisons for significant navigation_offloaded:obj_recognition_offloaded two-way interaction 

Since we have a significant two-way interaction between navigation and object recognition, we need to perform multiple pairwise comparison to determine which group means are different. We will use estimated marginal means test for this purpose with Bonferroni adjustment. We will group the results by object recognition, since the main effect of object recognition is significant, according to three-way ANOVA results:
```{r}
data %>% 
  group_by(obj_recognition_offloaded) %>%
  emmeans_test(size_of_packets ~ navigation_offloaded, p.adjust.method = "bonferroni", detailed = TRUE)
```
We can see that offloading navigation has a significant effect on the total size of network packets when object recognition is offloaded, but not when it is executed onboard.

## Results

In the box plot below, we can indeed see that when object recognition is offloaded, the total size of network packets is lower when navigation is offloaded. However, when object recognition is execution onboard, offloading navigation does not have significant effect on the total size of network packets. Finally, if we compare the means of two groups on the left and right, when object recognition is not offloaded and offloaded, we can see that offloading object recognition does have an enormous overall effect on the total size of network pakcets:
```{r}
pdf(file = "../../figures/unknown_map_experiment/size-of-packets-1.pdf")

ggplot(data, aes(x = obj_recognition_offloaded, y = size_of_packets, fill = navigation_offloaded)) +
  theme_bw() + 
  geom_boxplot(width = 0.9, outlier.size = 0.5) + 
  stat_summary(fun = mean, color = 'black', geom = 'point', shape = 5) +
  labs(x = "Object recognition offloaded", y = "Total size of network packets (MB)") +
  guides(fill=guide_legend(title="Navigation offloaded")) +
  theme(
    axis.title.x = element_text(size = 26, family = 'sans'),
    axis.title.y = element_text(size = 26, family = 'sans'),
    axis.text.x = element_text(size = 26, family = 'sans'),
    axis.text.y = element_text(size = 26, family = 'sans'),
    legend.title = element_text(size = 20, family = 'sans'),
    legend.text = element_text(size = 18, family = 'sans'),
    legend.position = "top"
  )

dev.off()
```